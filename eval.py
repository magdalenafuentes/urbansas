"""
Copyright (c) 2022 Magdalena Fuentes, Bea Steers, Luca Bondi(Robert Bosch GmbH), Julia Wilkins
All rights reserved.

This source code is licensed under the BSD-3-Clause license found in the
LICENSE file in the root directory of this source tree.
"""
import json
import os
from typing import List, Sequence, Iterable, Literal

import h5py
import glob
import librosa
import numpy as np
import pandas as pd
from pathlib import Path
import skimage.transform
from moviepy.editor import VideoFileClip
from matplotlib.patches import Rectangle

SR = 48000
MAX_LEN_ID = 11
IMAGE_SHAPE = (224, 224)

"""
A series of helper functions for evaluating these models, mainly 
IOU/GIOU scoring and video/audio pre-procesing methods.
"""


# Video helper functions
def video_to_size(cm: np.ndarray, shape=IMAGE_SHAPE) -> np.ndarray:
    """
    Converts a mask representation of a video to a specified size.

    Arguments:
        cm: original array to be converted into `shape` size.
        shape: desired image shape.

    Returns:
        np.ndarray: resized version of input
    """

    return skimage.transform.resize(cm, shape, order=0, preserve_range=True, anti_aliasing=False)


def video2audio_fname(fname: str, ext: str = None) -> str:
    """
    Gets the audio filename version of a video filename.
    Ex: convert: tau2021aus/video/street_traffic-barcelona-161-4901.mp4
        to: tau2021aus/audio/street_traffic-barcelona-161-4901.wav

    Arguments:
        fname: presumably a video filename.
        ext: specified audio filename extension (i.e. 'wav')

    Returns:
        str: audio filename with the specified extension.
    """
    fdir, _, fbase = fname.rsplit('/', 2)
    fstem = fbase.split('.')[0]
    fname = os.path.join(fdir, 'notebook/audio', fstem)
    if not ext:
        fname += '.*'
        fs = glob.glob(fname)
        assert fs, f'No files matching {fname}'
        return fs[0]
    return fname + ext


def load_sample(vid_fname: str, aud_fname: str, hop_size: float = 0.1,
                sr: int = SR, **kw):
    """
    Load an audio and video clip.

    Arguments:
        vid_fname: video filename.
        aud_fname: audio filename.
        hop_size: hop size for padding the audio.
        sr: sr used with hop size to pad audio.

    Returns:
        np.ndarray, np.ndarray: video and audio arrays.
    """

    win_size = sr
    y, _ = librosa.load(aud_fname, sr=sr)
    aud = librosa.util.frame(
        librosa.util.pad_center(y, int((len(y) + win_size - 1))),
        frame_length=sr, hop_length=int(sr * hop_size)).T[:, None]

    clip = VideoFileClip(vid_fname)

    vid = np.array([
        video_to_size(clip.get_frame(i), **kw)
        for i in np.linspace(0, len(y) / sr, len(aud))])
    vid = vid / 255 * 2 - 1
    return vid, aud


def load_av_data_from_files(video_fnames: List[str], audio_fnames: List[str], **kw):
    """
    Loads all audio and video data.

    Arguments:
        video_fnames: list of video filenames.
        audio_fnames: list of audio filesnames.

    Yields:
        np.ndarray, np.ndarray: loaded video and audio for each
            filename in list.
    """

    for vf, af in zip(video_fnames, audio_fnames):
        yield load_sample(vf, af, **kw)


# Loader helper functions
def load_predictions(predictions_path: str):
    """
    Load the predictions generated by the model. It expects folder structure to be:
    predictions_path
        - fold_0
            - prediction_fold0.h5
        - fold_1
            ....

    Arguments:
        predictions_path: path to predictions folder (see above)

    Returns:
        dict: nested dictinonary of the structure:
            {'100': {'time': [],
                     'pred': [],
                     'model': model_filepath}}, ..}
    """
    model_predictions = np.array(glob.glob(os.path.join(predictions_path, "*", "*.h5")))

    pred = {}
    for file_path in model_predictions:
        print(file_path)
        with h5py.File(file_path, 'r') as f:
            for uid in f.keys():
                pred[uid] = {
                    'time': np.array(f[uid]['time']),
                    'pred': np.array(f[uid]['pred']),
                    'model': file_path
                }
    return pred


def load_index(index_filepath: str):
    """
    Load the index (JSON file).

    Arguments:
        index_filepath: path to index JSON file.

    Returns:
        dict: dict containing index data.
    """
    index_path = Path(os.path.join('', index_filepath))  # PASS IN YOUR INDEX FILEPATH HERE
    with index_path.open() as f:
        index = json.load(f)
    print("Length of index: ", len(index))
    return index


def create_mask(coords: Sequence, existing_mask: np.ndarray = None, frame_size=(720, 1280)):
    """
    Generate a mask given [x1,y1,x2,y2] coordinates.

    Arguments:
        coords: [x1,y1,x2,y2] coordinates of top-left and bottom right corner of box.
        existing_mask: if we have an existing mask we will update with the new coordinates we
            can pass this here, otherwise a mask of zeroes will be initialized.
        frame_size: size of overall frame. a new mask created will be of this shape.

    Returns:
        np.ndarray: binary np.ndarray mask
    """

    if existing_mask is not None:
        if existing_mask.shape != frame_size:
            raise ValueError(
                f"Existing mask is of shape {existing_mask.shape}, but target frame size is {frame_size} These must match.")
        mask = existing_mask
    else:
        mask = np.zeros(frame_size)

    if not np.any(coords) or not coords:
        return np.zeros(frame_size)

    curr_x = (coords[0], coords[2])
    curr_y = (coords[1], coords[3])
    mask[curr_y[0]:curr_y[1] + 1, curr_x[0]:curr_x[1] + 1] = 1
    return mask


# IOU Scores
def iou_score(pred_mask: np.ndarray, gt_bbox_coords: Iterable[np.ndarray] = None, gt_box_mask: np.ndarray = None,
              th: float = 0.5, frame_size=(720, 1280), target_size=None):
    """
    Computes Intersection over Union (IoU) for a given mask and bounding boxes.
    Assumes that bboxes are according to original size, and box is in format [x1, y1, x2, y2]
    where (x1,y1) is the upper left corner and (x2,y2) the lower right corner. This is for
    one *class* at a time currently.

    pred_mask:
        mask thats the size of the entire image (likelihood), predictions for one frame
    gt_bbox_coords:
        list of coordinates from ground truth [x1, y1, x2, y2]
    gt_box_mask:
        ground truth box mask (if passed directly instead of creating)
    th:
        threshold tau that determines how to binarize the predicted max (which has likelihood vals)
    frame_size:
        size of the input frames (should be the same for pred/gt)
    target_size:
        desired output frame size of both masks

    Returns:
        (iou, gt_box_mask, pred_mask):
            iou: IoU score computed (float)
            gt_box_mask: reshaped ground truth box mask (2D np.ndarray, shape=target_size)
            pred_mask: reshaped prediction mask (2D np.ndarray, shape=target_size)
    """

    if gt_bbox_coords is None and gt_box_mask is None:
        raise ValueError("Need to pass either box or box mask")

    if len(pred_mask.shape) > 2:
        pred_mask = pred_mask[..., 0]

    # Convert the likelihood thresholds to a binary mask
    pred_mask = (pred_mask > th) ** 1

    if target_size is None:
        target_size = frame_size

    if gt_box_mask is None:
        print('creating box mask')
        gt_box_mask = np.zeros(frame_size)

        # Make the overall mask
        for _box in gt_bbox_coords:
            if _box is None or sum(_box) == 0:
                continue
            # +1 is to take the "borders" into account
            gt_box_mask = create_mask(coords=_box, existing_mask=gt_box_mask, frame_size=frame_size)

        gt_box_mask = video_to_size(gt_box_mask, target_size)

    # We can adjust resolution if necessary
    pred_mask = video_to_size(pred_mask, target_size)

    # Intersection, multiply and sum the 1/0s
    overlap = np.sum(pred_mask * gt_box_mask)

    # Union: elementwise subtraction
    union = np.sum((pred_mask - gt_box_mask > 0) + gt_box_mask)
    iou = overlap / union if union else 1  # This is where 1 is assigned to empty frames
    # Empty -> doesn't have GT and didn't predict anything
    return iou, gt_box_mask, pred_mask


def iou_frame(annot, pred, frame_id: int, **kw):
    """
    Computes the IoU score for a single frame.

    Arguments:
        annot: video annotations for one video
        pred: predictions for one video
        frame_id: single frame ID, this allows us to filter down to evaluate only
            one frame at a time

    Returns:
        See return for `iou_score`. Returns this for the given `frame_id`.
    """

    _annot = annot[annot.frame_id == frame_id]
    gth = []
    for _, a in _annot.iterrows():
        # ground truth
        # this converts GT to x1,y1 x2,y2
        # the predictions
        gth.append(np.array([a.x, a.y, a.x + a.w, a.y + a.h]).astype(int))
    return iou_score(pred, gth, **kw)  # each row in annotations -> one BB (in that frame)


def iou_frame_1D(annot, pred, frame_id, **kw):
    """
    Computes the IoU score for a single frame, but for height=1 boxes. We use this
    as we evaluate the vertical regions of frames.

    Arguments:
        annot: video annotations for one video
        pred: predictions for one video
        frame_id: single frame ID, this allows us to filter down to evaluate only
            one frame at a time
    Returns:
        See return for `iou_score`. Returns this for the given `frame_id`.
    """

    _annot = annot[annot.frame_id == frame_id]
    gth = []
    for _, a in _annot.iterrows():
        gth.append(np.array([a.x, 0, a.x + a.w, 1]).astype(int))
    return iou_score(pred, gth, **kw)


def score_file(index_data: dict,
               predictions_data: dict,
               uid: str,
               tau: float,
               pointwise_or_boxwise: Literal['pointwise', 'boxwise'],
               class_list: List[str],
               frame_type: str = 'all',
               labels_period: float = 0.5,
               fov: float = 120,
               num_regions: int = 5,
               file_duration: float = 10,
               verbose: bool = False) -> pd.DataFrame:
    """
    Get a dataframe of per-class IOU and GIOU scores for a given file UID.

    Arguments:
        index_data: Ground truth data (dict), indexed by UID.
        predictions_data: Predictions data (dict), indexed by UID.
        uid: File UID (found both in the index and prediction file)
        tau: Threshold used to binarize predictions.
        pointwise_or_boxwise: 'pointwise' or 'boxwise' evaluation strategy
        class_list: List of classes for the model.
        frame_type: Type of frames to evaluate for this file.
            One of:
                -'all' (includes active and inactive frames)
                -'active' (ground truth has something)
                -'inactive' (ground truth is empty)
        labels_period: number of annotations per second.
        fov: video field of view.
        num_regions: number of regions in which the frame is horizontally split.
        file_duration: file duration [s]
        verbose: If true, print more details about the frame-by-frame scoring.

    Returns:
        pd.DataFrame: dataframe of per-class IOU and GIOU scores for a given file UID.
    """

    file_index = index_data[uid]
    file_pred = predictions_data[uid]
    overall_per_class_scores = []
    overall_per_class_scores_giou = []
    invalid_files = []
    fps = 1 / labels_period

    # Frames, classes, regions
    f, c, r = file_pred['pred'].shape
    mask_per_class = file_pred['pred'].reshape((c, f, r))

    # print(f, c, r)
    # print(file_duration/labels_period)
    if f < file_duration / labels_period:
        # print(f,c,r)
        return

    # Initialize scores dict for this file
    class_score = {c: None for c in class_list}
    giou_class_score = {c: None for c in class_list}

    # Loop through the classes present in this file
    for class_index, class_name in enumerate(class_list):
        if verbose:
            print("CURRENT CLASS: ", class_name)

        # Get all events of this class
        events_for_this_class = [d for d in file_index['events'] if d['label'] == class_name]

        frame_scores = []
        giou_frame_scores = []

        # Loop through time
        for f, t in enumerate(np.arange(0, file_duration, 1 / fps)):
            # For a given time frame, collect all indexes of this class and their azimuth indexes

            # Initialize box of region size (1D) - one box per frame
            box = np.zeros((1, r))

            # For each event in this class
            for event in events_for_this_class:
                if event['source'] == 'video':

                    # Basically a way of checking to see if the current events time
                    # array matches with a loop through possible timeframes
                    if any(np.abs(t - event['time']) < 1 / fps):

                        # Get index to look for in azmith array
                        idx = np.argmin(np.abs(t - event['time']))

                        if pointwise_or_boxwise == 'boxwise':
                            # BOXWISE SCORING
                            event_aximuth_idxs_r = np.round(
                                (event['azimuth_right'][idx] + fov / 2) / fov * (num_regions - 1)).astype(int)
                            event_aximuth_idxs_l = np.round(
                                (event['azimuth_left'][idx] + fov / 2) / fov * (num_regions - 1)).astype(int)
                            try:
                                box[:, event_aximuth_idxs_l:event_aximuth_idxs_r + 1] = 1
                            except IndexError:
                                invalid_files.append(uid)
                        elif pointwise_or_boxwise == 'pointwise':
                            # POINTWISE SCORING
                            event_azimuth_idxs = np.round(
                                (event['azimuth'][idx] + fov / 2) / fov * (num_regions - 1)).astype(int)
                            try:
                                box[:, event_azimuth_idxs] = 1
                            except IndexError:
                                invalid_files.append(uid)
                        else:
                            raise Exception("pointwise_or_boxwise argument should be one of ['pointwise', 'boxwise']")

            if verbose:
                print("TIME: ", t)
                print("GT BOX MASK: ", box)
                print("PRED BOX MASK: ", (mask_per_class[
                                              class_index, f, None] > tau) ** 1)
                print("PRED BOX MASK NO THRESHOLD: ", (mask_per_class[class_index, f, None]))

            # One score per frame per class
            iou_res = iou_score(pred_mask=mask_per_class[class_index, f, None],  # the pred is many instances
                                gt_bbox_coords=[],
                                gt_box_mask=box,
                                th=tau,
                                frame_size=(1, r))

            giou_res = giou_score(pred_mask=(mask_per_class[class_index, f, None] > tau) ** 1,
                                  # the pred is many instances
                                  gt_mask=box,
                                  th=tau,
                                  frame_size=(1, r))

            if verbose:
                print("IOU Score: ", iou_res[0])
                print("GIOU Score: ", giou_res[0], "\n")

            # Add this to the scores list
            # Inactive
            if frame_type == 'all':
                frame_scores.append(iou_res[0])
                giou_frame_scores.append(giou_res[0])
            elif frame_type == 'active':
                if not np.all((iou_res[1] == 0)):
                    frame_scores.append(iou_res[0])
                    giou_frame_scores.append(giou_res[0])
            elif frame_type == 'inactive':
                if np.all((iou_res[1] == 0)):
                    frame_scores.append(iou_res[0])
                    giou_frame_scores.append(giou_res[0])
            else:
                raise ValueError('frame_type should be "active", "inactive", or "all"')

        class_score[class_name] = frame_scores
        giou_class_score[class_name] = giou_frame_scores

    avgs = [np.mean(element) for element in class_score.values()]
    overall_per_class_scores.append(avgs)

    avgs_giou = [np.mean(element) for element in giou_class_score.values()]
    overall_per_class_scores_giou.append(avgs_giou)

    cols = class_list + ['score', 'tau', 'frame_type', 'uid', 'model']
    overall_per_class_scores[0] += ['iou']
    overall_per_class_scores[0] += [tau]
    overall_per_class_scores[0] += [frame_type]
    overall_per_class_scores[0] += [uid]
    overall_per_class_scores[0] += [file_pred['model']]
    overall_per_class_scores_giou[0] += ['giou']
    overall_per_class_scores_giou[0] += [tau]
    overall_per_class_scores_giou[0] += [frame_type]
    overall_per_class_scores_giou[0] += [uid]
    overall_per_class_scores_giou[0] += [file_pred['model']]

    iou_scoring_df = pd.DataFrame(overall_per_class_scores, columns=cols)
    giou_scoring_df = pd.DataFrame(overall_per_class_scores_giou, columns=cols)

    final = pd.concat([iou_scoring_df, giou_scoring_df])
    return final


def iou_video(annot, corrs, **kw):
    """
    Compute an array of frame by frame IOU scores for a video.

    annot:
        video annotations for one video
    corr:

    Returns:
        See return for `iou_frame` and `iou_score`. Returns this for each frame in a video.
    """
    return [iou_frame(annot, pred, frame_id, **kw) for frame_id, pred in enumerate(corrs, 1)]


def eval_video(vfname: str, annotations, corrs, **kw):
    """
    Compute an array of frame-by-frame IOU scores given a video filename.

    vfname:
        video filename
    annotations:
        video annotations for the given video filename.
    corrs:

    Returns:
        See return for `iou_video`. Gets final evaluation for a video.
    """

    if vfname not in annotations.filename.unique():
        raise ValueError(f'{vfname} is not a valid video name in the dataset')
    vannot = annotations[annotations.filename == vfname]
    return iou_video(vannot, corrs, **kw)


## GIOU Code
def get_enclosing_mask(mask1: np.ndarray, mask2: np.ndarray):
    """
    Generate coordinates [x1,y1,x2,y2] of the smallest enclosing convex
    rectangle that encloses the given two masks.

    mask1:
        binary np.ndarray (1D or 2D+) representing a mask
    mask2:
        binary np.ndarray (1D or 2D+) representing a mask

    Returns:
        coords: [x1,y1,x2,y2] list of coordinates of smallest enclosing mask.
    """
    # Where these are 2D arrays of the two masks
    mask1_min_xy = min(list(zip(*np.where(mask1 == 1))))
    mask1_max_xy = max(list(zip(*np.where(mask1 == 1))))

    mask2_min_xy = min(list(zip(*np.where(mask2 == 1))))
    mask2_max_xy = max(list(zip(*np.where(mask2 == 1))))

    min_x = min(mask1_min_xy[1], mask2_min_xy[1])
    min_y = min(mask1_min_xy[0], mask2_min_xy[0])

    max_x = max(mask1_max_xy[1], mask2_max_xy[1])
    max_y = max(mask1_max_xy[0], mask2_max_xy[0])

    coords = [min_x, min_y, max_x, max_y]
    return coords


def giou_score(pred_mask: np.ndarray, gt_mask: np.ndarray, th: float = 0.5,
               frame_size=(720, 1280), ):
    """
    Computes Generalized Intersection over Union (GIoU) score for a given predicted
    mask and ground truth mask. **NOTE** that this metric is currently only designed to
    work when there is only one bounding box in a frame.

    The score itself is IoU - (C(AUB)/C)

    pred_mask:
        prediction mask for one frame, (not binary - it will be in likelihood)
    gt_mask_coords:
        coordinates of ground truth bbox [x1, y1, x2, y2]
    th:
        threshold tau that determines how to binarize the predicted max (which has likelihood vals)
    frame_size:
        size of the input frames (should be the same for pred/gt)

    Returns:
        (giou, gt_mask, pred_mask, c_mask):
            iou: IoU score computed (float)
            gt_mask: reshaped ground truth box mask (2D np.ndarray, shape=target_size)
            pred_mask: reshaped prediction mask (2D np.ndarray, shape=target_size)
            c_mask: reshaped mask of smallest rectangle enclosing gt_mask and pred_mask.
    """

    # Convert coordinates to mask right away
    # If both the prediction and the ground truth are empty (all zeros)
    if not np.any(pred_mask) and not np.any(gt_mask):
        return 1, gt_mask, pred_mask, create_mask([0, 0, 0, 0], frame_size=frame_size)

    # One is empty the other is not
    if (not np.any(pred_mask) and np.any(gt_mask)) or (not np.any(gt_mask) and np.any(pred_mask)):
        return 0, gt_mask, pred_mask, create_mask([0, 0, 0, 0], frame_size=frame_size)

    # Convert the likelihood thresholds to a binary mask
    pred_mask = (pred_mask > th) ** 1

    # gt_mask = video_to_size(gt_mask, target_size)
    # pred_mask = video_to_size(pred_mask, target_size)

    overlap = np.sum(pred_mask * gt_mask)
    union = np.sum((pred_mask - gt_mask > 0) + gt_mask)

    # Get enclosing mask
    get_c = get_enclosing_mask(gt_mask, pred_mask)  # This gets (x,y) top left, width, height
    c_mask = create_mask(coords=get_c,
                         frame_size=frame_size)
    c_diff = np.sum(c_mask - ((pred_mask - gt_mask > 0) + gt_mask))
    res = np.abs(c_diff) / np.abs(np.sum(c_mask))

    iou = overlap / union
    giou = iou - res

    return giou, gt_mask, pred_mask, c_mask


def giou_frame(annot: pd.DataFrame, pred: np.ndarray, frame_id: int, **kw):
    """
    Computes the GIoU score for a single frame.
    **Note** that this is only designed for single bounding boxes per frames.

    annot:
        video annotations for one video
    pred:
        predictions for one video
    frame_id:
        single frame ID, this allows us to filter down to evaluate only
        one frame at a time

    Returns:
        See return for `giou_score`. Returns this for the given `frame_id`.
    """

    _annot = annot[annot.frame_id == frame_id].iloc[0]
    gth = np.array([_annot.x, _annot.y, _annot.x + _annot.w, _annot.y + _annot.h]).astype(int)
    return giou_score(pred, gth, **kw)


def giou_frame_1D(annot: pd.DataFrame, pred: np.ndarray, frame_id: int, **kw):
    """
    Computes the GIoU score for a single frame, but for height=1 boxes. We use this
    as we evaluate the vertical regions of frames.
     **Note** that this is only designed for single bounding boxes per frames.

    Arguments:
        annot: video annotations for one video
        pred:  predictions for one video
        frame_id: single frame ID, this allows us to filter down to evaluate only
            one frame at a time

    Returns:
        See return for `iou_score`. Returns this for the given `frame_id`.
    """
    _annot = annot[annot.frame_id == frame_id].iloc[0]
    gth = np.array([_annot.x, 0, _annot.x + _annot.w, 1]).astype(int)
    return giou_score(pred, gth, **kw)


def giou_video(annot: pd.DataFrame, corrs, **kw):
    """
    Compute an array of frame by frame GIOU scores for a video.

    Arguments:
        annot: video annotations for one video
        corrs:

    Returns:
        See return for `iou_frame` and `iou_score`. Returns this for each frame in a video.
    """
    return [giou_frame(annot, pred, frame_id, **kw) for frame_id, pred in enumerate(corrs, 1)]


def giou_eval_video(vfname: str, annotations: pd.DataFrame, corrs, **kw):
    """
    Compute an array of frame-by-frame GIOU scores given a video filename.

    Arguments:
        vfname: video filename
        annotations: video annotations for the given video filename.
        corrs:

    Returns:
        See return for `giou_video`. Gets final evaluation for a video.
    """

    if vfname not in annotations.filename.unique():
        raise ValueError(f'{vfname} is not a valid video name in the dataset')
    vannot = annotations[annotations.filename == vfname]
    return giou_video(vannot, corrs, **kw)


def get_x1y1x2y2(mask: np.ndarray):
    """
    Given a binary mask, calculate [x1,y1,x2,y2] coordinate form of
    this mask in a format convenient for plotting.

    mask:
        binary np.ndarray (1D or 2D+) representing a mask

    Returns:
        (min_x, min_y): bottom left corner coordinates
        width: x-dimension width of box
        height: y-dimension height of box
    """

    xy = list(zip(*np.where(mask == 1)))
    # If the mask is in 2d array form
    min_x = min([a[0] for a in xy])
    min_y = min([a[1] for a in xy])
    max_x = max([a[0] for a in xy])
    max_y = max([a[1] for a in xy])

    width = max_x - min_x
    height = max_y - min_y
    return (min_x, min_y), width, height


def get_random_prediction(n_frames, n_classes, n_regions, point_sources=True, fps=2):
    """
    Creates a random prediction as baseline.

    Args:
        n_frames: number of frames the prediction should have.
        n_classes: number of classes the prediction should have.
        n_regions: number of regions (divisions of fov).
        point_sources: if True the random predictions are created mimicking a
                        poitwise model, if False they mimic a boxwise model.

    Returns:
        dict:
            pred: (n_frames, n_classes, n_regions) with the random predictions
            model: name of the model
            time: times of random predictions
    """

    shape = (n_frames, n_classes, n_regions)
    predictions = np.zeros(shape)

    if point_sources:
        max_rand_reg = 2
    else:
        max_rand_reg = 5

    for f in range(n_frames):
        for r in range(np.random.randint(0, max_rand_reg + 1)):
            c = np.random.randint(0, n_classes)
            r_ = np.random.randint(0, n_regions)
            predictions[f, c, r_] = 1

    pred_dict = {'pred': predictions,
                 'model': f'random_{"pointwise" if point_sources else "boxwise"}',
                 'time': np.arange(n_frames) / fps}


    return pred_dict